model:
  vocab_size: 50257
  d_model: 256
  num_heads: 4
  num_layers: 3
  d_ff: 1024
  max_seq_len: 256
  dropout: 0.2

training:
  batch_size: 16
  num_epochs: 15
  learning_rate: 0.0003
  patience: 2

data:
  dataset_name: wikitext
  dataset_config: wikitext-2-raw-v1